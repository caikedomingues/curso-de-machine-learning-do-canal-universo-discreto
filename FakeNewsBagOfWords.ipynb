{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61cd610b",
   "metadata": {},
   "source": [
    "                                word embeddings\n",
    "-> Word Embeddings são representações numéricas densas de palavras. Em termos\n",
    "simples, eles transformam palavras (que são dados categóricos/texto) em vetores\n",
    "de números reais (dados numéricos) em um espaço de alta dimensão.\n",
    "\n",
    "                        A ideia Principal: O vetor Significa Contexto\n",
    "-> O objetivo principal dos embeddings é representar o significado de uma palavra pelo o seu contexto. Palavras que são usadas em contextos semelhantes\n",
    "(e, portanto, têm significados semelhantes) terão vetores embedding próximos\n",
    "umas das outras no espaço vetorial.\n",
    "\n",
    "                        Como Eles Funcionam (Propriedades Chave)\n",
    "-> Dimensionalidade Reduzida e Densa: Em vez de usar a técnica antiga de \"One-Hot Encoding\" (onde o vetor teria milhares de zeros), um embedding\n",
    "tipico tem apenas algumas centenas de dimensões (ex: 100, 300 ou 768). O\n",
    "vetor é denso porque a maioria dos seus não é zero.\n",
    "\n",
    "-> Captura de Relacionamentos Semânticos: Essa é a parte mágica. Os embeddings\n",
    "são treinados para capturar a relação entre palavras. Por exemplo: Se pegarmos\n",
    "o vetor da palavra rei e substrair o vetor de homem, e depois somar o vetor de\n",
    "mulher, o vetor resultante será muito próximo ao vetor da palavra rainha\n",
    "\n",
    "Vetor(Rei) - Vetor(Homem) + Vetor(Mulher) = Vetor(Rainha).\n",
    "\n",
    "-> Eles capturam relações como capital-pais, verbo-tempo, e gênero.\n",
    "\n",
    "-> Os emebeddings não são criados manualmente, eles são aprendidos por modelos de machine learning (redes neurais) que processam grandes quantidades de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ed95b",
   "metadata": {},
   "source": [
    "                            Bag-of-Words (Saco de Palavras)\n",
    "-> É um modelo de representação de texto que ignora a gramatica e a ordem das\n",
    "palavras, focando apenas na frequência de ocorrência de cada palavra de um documento. Imagine um \"saco de palavras\" de onde você retira todas as palavras\n",
    "de um documento, mas as joga de volta sem se preocupar com a ordem em que foram\n",
    "retiradas.\n",
    "\n",
    "                                Como Funciona\n",
    "-> O processo de criação de representação Bag-of-Words envolve duas etapas principais:\n",
    "\n",
    "-> Criar o vocabulário (o \"Saco\"): O modelo primeiro escaneia todos os documentos no seu corpus (conjunto de textos) e cria uma lista de todas as palavras únicas que aparecem. Essa list é o vocabulário do modelo.\n",
    "\n",
    "Criar o Vetor de Frequência: Para cada documento individual. é criado um vetor. O tamanho desse vetor é igual ao tamanho do vocabulário. Cada posição\n",
    "(coluna) no vetor corresponde a uma palavra única do vocabulário.\n",
    "  O valor na posição é a contagem (frequência) de quantas vezes aquela palavra apareceu naquele documento especifico.\n",
    "\n",
    "                            Vantagens\n",
    "-> Simples e Rápido: Fácil de entender e de implementar (com CountVectorizer ou TfidVectorizer no scikit-learn).\n",
    "\n",
    "-> Escalável: Funciona bem mesmo com grandes volumes de texto\n",
    "\n",
    "-> Funciona com Classificadores Tradicionais: É a representação ideal para algoritmos como Regressão Logistica, Naive Bayes e SVM.\n",
    "\n",
    "\n",
    "                            Desvantagens\n",
    "-> Perde a ordem: Ignora a ordem das palavras, perdendo contexto crucial (ex: \"Bom, mas não muito\" vs \"Não é muito bom\").\n",
    "\n",
    "-> Vetor Esparso: Para vocabulários grandes (milhares de palavras), a maioria dos valores nos vetores será zero.\n",
    "\n",
    "-> Não captura Significado/Relação: Palavras com o mesmo significado (sinônimos) são tratadas como totalmente diferentes.\n",
    "\n",
    "-> Em resumo, Bag-Of-Words é um modelo que transforma documentos em vetores de\n",
    "frequência, permitindo que algoritmos de Machine Learning os processem, mas sacrifica o contexto e a ordem das palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76efb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "\n",
    "    'Esse é o primeiro documento',\n",
    "    \n",
    "    'Esse é o segundo',\n",
    "    \n",
    "    'Seria esse o primeiro?'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49cfd9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário:  {'esse': 1, 'primeiro': 2, 'documento': 0, 'segundo': 3, 'seria': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "print(\"Vocabulário: \", vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8913294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
